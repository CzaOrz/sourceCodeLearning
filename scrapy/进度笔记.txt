判断是否为scrapy项目环境，由当前环境往上递归查询，看是否能够找到scrapy.cfg配置文件
加载settings模块。如果SCRAPY_SETTINGS_MODULE这玩意不在so.environ环境里，则初始化他，首先找到配置文件scrapy.cfg，把他里面关于setting-default的值加载到系统环境中。然后使用Setting模块加载此配置文件的值，也就是那个路径
加载所有的commands指令，通过遍历scrapy.commands目录，加载所有的指令，到cmds的一个list里面，就是不听的import_module，iter_modules，进行一些逻辑上的判断，是否是子模块
解析从sys.argv系统环境获取的输入参数，检查是否符合规则，也就是指令是否正确，此时通过输入的指令，我们就可以定位一个确切的scrapy-command指令，还通过optparse包，获取其余参数，准备好执行环境
通过实例化一个CrawlerProcess对象，进行爬虫管理，然后执行command.run启动指令


第一个接触到start_requests的居然是爬虫中间件的process_start_requests函数，但是scrapy自身是没有实现任何处理的，他允许开发人员自己定义，会在所有scrapy函数优先级之上


爬虫是从start_requests开始的，首先不停的next从中取出Request请求，push到队列，然后循环时又立即pop出来，扔到下载器那边去，下载器下完后，会对结果进行检查，如果是Request则继续入队
若果是Response，则调用self.scraper.enqueue_scrape(response, request, spider) 进行处理
在这里首先会爬虫中间件，他会对爬虫的输入，和爬虫的输出进行处理，但是这里的传入的response只是从下载器那边下载过来的，还未经过回调函数处理
在执行完爬虫输入处理后，就会执行上一级的爬虫回调函数，进入下一级。最后输出时对结果进行检测，如爬虫输出为Request则继续入队，如果输出为字典则调用管道的process_item进行处理
